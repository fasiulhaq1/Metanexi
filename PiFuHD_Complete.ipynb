{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PiFuHD**"
      ],
      "metadata": {
        "id": "tgDNE-r1ZCwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2NY1EhEsrpOfzHL2d05uszWdmBz_2wzAHXkMx4iLvewqh97UX"
      ],
      "metadata": {
        "id": "3Cr9MmGikkjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2NY1EhEsrpOfzHL2d05uszWdmBz_2wzAHXkMx4iLvewqh97UX"
      ],
      "metadata": {
        "id": "6kMshCZlkaQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok flask_ngrok Flask-Cors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpECeFH5j3xr",
        "outputId": "496c93fd-4a66-4deb-8182-a3a8a4061b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting flask_ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting Flask-Cors\n",
            "  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.31.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok) (2.1.5)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pyngrok, flask_ngrok, Flask-Cors\n",
            "Successfully installed Flask-Cors-4.0.1 flask_ngrok-0.0.25 pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_GUmWiIhk3iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-image==0.21.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iGoiKYeZtz7",
        "outputId": "4ee1aede-4e1e-43ef-b51a-7ebc149789f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-image==0.21.0\n",
            "  Downloading scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0) (3.3)\n",
            "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0) (2024.7.24)\n",
            "Collecting PyWavelets>=1.1.1 (from scikit-image==0.21.0)\n",
            "  Downloading pywavelets-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0) (24.1)\n",
            "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0) (0.4)\n",
            "Downloading scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets, scikit-image\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.23.2\n",
            "    Uninstalling scikit-image-0.23.2:\n",
            "      Successfully uninstalled scikit-image-0.23.2\n",
            "Successfully installed PyWavelets-1.6.0 scikit-image-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/pifuhd\n",
        "!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git\n",
        "\n",
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n",
        "\n",
        "%cd /content/pifuhd/\n",
        "!sh ./scripts/download_trained_model.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePG5XuIZZu9j",
        "outputId": "2a6053af-6db7-4d95-888e-2b3c1627ab94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pifuhd'...\n",
            "remote: Enumerating objects: 222, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 222 (delta 92), reused 82 (delta 82), pack-reused 96\u001b[K\n",
            "Receiving objects: 100% (222/222), 399.35 KiB | 1.54 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n",
            "Cloning into 'lightweight-human-pose-estimation.pytorch'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 124 (delta 21), reused 19 (delta 18), pack-reused 90\u001b[K\n",
            "Receiving objects: 100% (124/124), 230.77 KiB | 1.21 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "/content/lightweight-human-pose-estimation.pytorch\n",
            "--2024-07-30 21:19:20--  https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n",
            "Resolving download.01.org (download.01.org)... 23.202.179.18, 2600:1406:bc00:f8a::a87, 2600:1406:bc00:f86::a87\n",
            "Connecting to download.01.org (download.01.org)|23.202.179.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87959810 (84M) [application/octet-stream]\n",
            "Saving to: ‘checkpoint_iter_370000.pth’\n",
            "\n",
            "checkpoint_iter_370 100%[===================>]  83.88M   107MB/s    in 0.8s    \n",
            "\n",
            "2024-07-30 21:19:22 (107 MB/s) - ‘checkpoint_iter_370000.pth’ saved [87959810/87959810]\n",
            "\n",
            "/content/pifuhd\n",
            "+ mkdir -p checkpoints\n",
            "+ cd checkpoints\n",
            "+ wget https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt pifuhd.pt\n",
            "--2024-07-30 21:19:22--  https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.25, 13.226.210.15, 13.226.210.111, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1548375177 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘pifuhd.pt’\n",
            "\n",
            "pifuhd.pt           100%[===================>]   1.44G   149MB/s    in 14s     \n",
            "\n",
            "2024-07-30 21:19:36 (104 MB/s) - ‘pifuhd.pt’ saved [1548375177/1548375177]\n",
            "\n",
            "--2024-07-30 21:19:36--  http://pifuhd.pt/\n",
            "Resolving pifuhd.pt (pifuhd.pt)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘pifuhd.pt’\n",
            "FINISHED --2024-07-30 21:19:36--\n",
            "Total wall clock time: 15s\n",
            "Downloaded: 1 files, 1.4G in 14s (104 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install iopath portalocker\n",
        "!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NxGgnORZxFT",
        "outputId": "06d296ba-2c8f-483d-89c8-676c8afa5aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath) (4.66.4)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.12.2)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: iopath\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31531 sha256=8e5c2c5f0f7364bff71e7695eef7323ac868cd8e02533355d1f2307e39e9f27b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built iopath\n",
            "Installing collected packages: portalocker, iopath\n",
            "Successfully installed iopath-0.1.10 portalocker-2.10.1\n",
            "Collecting git+https://github.com/facebookresearch/pytorch3d.git\n",
            "  Cloning https://github.com/facebookresearch/pytorch3d.git to /tmp/pip-req-build-gjsnd5hj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-gjsnd5hj\n",
            "  Resolved https://github.com/facebookresearch/pytorch3d.git to commit 7edaee71a9583f8f1eedf91187aa77a0797c38e0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorch3d==0.7.7) (0.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d==0.7.7) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d==0.7.7) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d==0.7.7) (2.10.1)\n",
            "Building wheels for collected packages: pytorch3d\n",
            "  Building wheel for pytorch3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch3d: filename=pytorch3d-0.7.7-cp310-cp310-linux_x86_64.whl size=57258553 sha256=e2ad05cb740e6df20a27dd97ea64e95f9cd133547deb5c6ec09983769250346a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wdkwlzck/wheels/dd/74/cc/b9266c863f19026f796e59a04e1cd9eb3754474a52ce1b66ce\n",
            "Successfully built pytorch3d\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjYp3BCJZzGc",
        "outputId": "ab8f20ab-98cd-49b1-c6fc-e9ab7a0a3588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lightweight-human-pose-estimation.pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "import demo\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def get_rect(net, images, height_size):\n",
        "    net = net.eval()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "    for image in images:\n",
        "        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n",
        "        img = cv2.imread(image, cv2.IMREAD_COLOR)\n",
        "        orig_img = img.copy()\n",
        "        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n",
        "\n",
        "        total_keypoints_num = 0\n",
        "        all_keypoints_by_type = []\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "        current_poses = []\n",
        "\n",
        "        rects = []\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=int) * -1\n",
        "            valid_keypoints = []\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n",
        "            valid_keypoints = np.array(valid_keypoints)\n",
        "\n",
        "            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n",
        "                pmin = valid_keypoints.min(0)\n",
        "                pmax = valid_keypoints.max(0)\n",
        "                center = (0.5 * (pmax[:2] + pmin[:2])).astype(int)\n",
        "                radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n",
        "            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n",
        "                # if leg is missing, use pelvis to get cropping\n",
        "                center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(int)\n",
        "                radius = int(1.45 * np.sqrt(((center[None, :] - valid_keypoints) ** 2).sum(1)).max(0))\n",
        "                center[1] += int(0.05 * radius)\n",
        "            else:\n",
        "                center = np.array([img.shape[1] // 2, img.shape[0] // 2])\n",
        "                radius = max(img.shape[1] // 2, img.shape[0] // 2)\n",
        "\n",
        "            x1 = center[0] - radius\n",
        "            y1 = center[1] - radius\n",
        "\n",
        "            rects.append([x1, y1, 2 * radius, 2 * radius])\n",
        "\n",
        "        np.savetxt(rect_path, np.array(rects), fmt='%d')\n",
        "\n",
        "# Define the path to your image\n",
        "image_path = 'D:/Ahmer/project/For backend Latest_002/uploads/best_frame_no_bg.png'  # Update this path to your actual image path\n",
        "\n",
        "# Empty the folder\n",
        "!rm -rf 'D:/Ahmer/project/For backend Latest_002/pifuhd/sample_images'\n",
        "!mkdir 'D:/Ahmer/project/For backend Latest_002/pifuhd/sample_images'\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread(image_path)\n",
        "cv2.imwrite('D:/Ahmer/project/For backend Latest_002/pifuhd/sample_images/Img.png', img)\n",
        "\n",
        "import os\n",
        "image_path = 'D:/Ahmer/project/For backend Latest_002/pifuhd/sample_images/Img.png'\n",
        "image_dir = os.path.dirname(image_path)\n",
        "file_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "# Output paths\n",
        "obj_path = 'D:/Ahmer/project/For backend Latest_002/pifuhd/results/pifuhd_final/recon/result_%s_256.obj' % file_name\n",
        "out_img_path = 'D:/Ahmer/project/For backend Latest_002/pifuhd/results/pifuhd_final/recon/result_%s_256.png' % file_name\n",
        "video_path = 'D:/Ahmer/project/For backend Latest_002/pifuhd/results/pifuhd_final/recon/result_%s_256.mp4' % file_name\n",
        "video_display_path = 'D:/Ahmer/project/For backend Latest_002/pifuhd/results/pifuhd_final/result_%s_256_display.mp4' % file_name\n",
        "\n",
        "# Preprocessing\n",
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "\n",
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "get_rect(net.cuda(), [image_path], 512)\n",
        "\n",
        "# Run\n",
        "%cd /content/pifuhd/\n",
        "# Warning: all images with the corresponding rectangle files under -i will be processed.\n",
        "!python -m apps.simple_test -r 256 --use_rect -i $image_dir\n",
        "# Seems that 256 is the maximum resolution that can fit into Google Colab.\n",
        "# If you want to reconstruct a higher-resolution mesh, please try with your own machine.\n",
        "\n",
        "# Clear everything\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "# Render video\n",
        "from lib.colab_util import generate_video_from_obj, set_renderer, video\n",
        "\n",
        "renderer = set_renderer()\n",
        "generate_video_from_obj(obj_path, out_img_path, video_path, renderer)\n",
        "\n",
        "# We cannot play a mp4 video generated by cv2\n",
        "!ffmpeg -i $video_path -vcodec libx264 $video_display_path -y -loglevel quiet\n",
        "video(video_display_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "cC5zFalQZ1GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Taking Transparent Image To Convert**"
      ],
      "metadata": {
        "id": "1YRCvW2EaZoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your image\n",
        "image_path = '/content/best_frame_no_bg.png'  # Update this path to your actual image path\n",
        "\n",
        "# Empty the folder\n",
        "!rm -rf '/content/pifuhd/sample_images'\n",
        "!mkdir '/content/pifuhd/sample_images'\n",
        "\n",
        "import cv2\n",
        "img = cv2.imread(image_path)\n",
        "cv2.imwrite('/content/pifuhd/sample_images/Img.png', img)\n",
        "\n",
        "import os\n",
        "image_path = '/content/pifuhd/sample_images/Img.png'\n",
        "image_dir = os.path.dirname(image_path)\n",
        "file_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "# Output paths\n",
        "obj_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.obj' % file_name\n",
        "out_img_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.png' % file_name\n",
        "video_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.mp4' % file_name\n",
        "video_display_path = '/content/pifuhd/results/pifuhd_final/result_%s_256_display.mp4' % file_name\n",
        "\n",
        "# Preprocessing\n",
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "\n",
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "get_rect(net.cuda(), [image_path], 512)\n",
        "\n",
        "# Run\n",
        "%cd /content/pifuhd/\n",
        "# Warning: all images with the corresponding rectangle files under -i will be processed.\n",
        "!python -m apps.simple_test -r 256 --use_rect -i $image_dir\n",
        "# Seems that 256 is the maximum resolution that can fit into Google Colab.\n",
        "# If you want to reconstruct a higher-resolution mesh, please try with your own machine.\n",
        "\n",
        "# Clear everything\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "# Render video\n",
        "from lib.colab_util import generate_video_from_obj, set_renderer, video\n",
        "\n",
        "renderer = set_renderer()\n",
        "generate_video_from_obj(obj_path, out_img_path, video_path, renderer)\n",
        "\n",
        "# We cannot play a mp4 video generated by cv2\n",
        "!ffmpeg -i $video_path -vcodec libx264 $video_display_path -y -loglevel quiet\n",
        "video(video_display_path)\n"
      ],
      "metadata": {
        "id": "Fc_j4YmUZ3b7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/pifuhd/results/pifuhd_final/recon/result_Img_256.obj')"
      ],
      "metadata": {
        "id": "4txwdbG0aq7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hAWXFR0WkzZn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}